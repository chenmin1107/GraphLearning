{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import mean_squared_error\n",
    "import dgl\n",
    "from dgl.data.rdf import AIFBDataset, MUTAGDataset, BGSDataset, AMDataset\n",
    "from dgl.nn.pytorch import RelGraphConv\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_nodes, h_dim)\n",
    "        # two-layer RGCN\n",
    "        self.conv1 = RelGraphConv(h_dim, h_dim, num_rels, regularizer='basis',\n",
    "                                  num_bases=num_rels, self_loop=False)\n",
    "        self.conv2 = RelGraphConv(h_dim, out_dim, num_rels, regularizer='basis',\n",
    "                                  num_bases=num_rels, self_loop=False)\n",
    "\n",
    "    def forward(self, g):\n",
    "        x = self.emb.weight\n",
    "        h = F.relu(self.conv1(g, x, g.edata[dgl.ETYPE], g.edata['norm']))\n",
    "        h = self.conv2(g, h, g.edata[dgl.ETYPE], g.edata['norm'])\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(g, target_idx, labels, test_mask, model):\n",
    "    test_idx = torch.nonzero(test_mask, as_tuple=False).squeeze()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g)\n",
    "    logits = logits[target_idx]\n",
    "    return mean_squared_error(logits[test_idx].squeeze(1), labels[test_idx])\n",
    "\n",
    "def train(g, target_idx, labels, train_mask, model):\n",
    "    # define train idx, loss function and optimizer\n",
    "    train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "#     loss_fcn = nn.CrossEntropyLoss()\n",
    "    # Regression\n",
    "    loss_fcn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=5e-4)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(50):\n",
    "        logits = model(g)\n",
    "        logits = logits[target_idx]\n",
    "        loss = loss_fcn(logits[train_idx].squeeze(1), labels[train_idx].float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        err = mean_squared_error(logits[train_idx].squeeze(1), labels[train_idx])\n",
    "        print(\"Epoch {:05d} | Loss {:.4f} | Train MSE {:.4f} \"\n",
    "              . format(epoch, loss.item(), err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with DGL built-in RGCN module.\n",
      "Done loading data from cached files.\n",
      "Epoch 00000 | Loss 8.6259 | Train MSE 8.6259 \n",
      "Epoch 00001 | Loss 89.5997 | Train MSE 89.5997 \n",
      "Epoch 00002 | Loss 6.8727 | Train MSE 6.8727 \n",
      "Epoch 00003 | Loss 6.1834 | Train MSE 6.1834 \n",
      "Epoch 00004 | Loss 10.0802 | Train MSE 10.0802 \n",
      "Epoch 00005 | Loss 6.2293 | Train MSE 6.2293 \n",
      "Epoch 00006 | Loss 2.4950 | Train MSE 2.4950 \n",
      "Epoch 00007 | Loss 1.0117 | Train MSE 1.0117 \n",
      "Epoch 00008 | Loss 0.9046 | Train MSE 0.9046 \n",
      "Epoch 00009 | Loss 1.2232 | Train MSE 1.2232 \n",
      "Epoch 00010 | Loss 1.4605 | Train MSE 1.4605 \n",
      "Epoch 00011 | Loss 1.4417 | Train MSE 1.4417 \n",
      "Epoch 00012 | Loss 1.2370 | Train MSE 1.2370 \n",
      "Epoch 00013 | Loss 0.9955 | Train MSE 0.9955 \n",
      "Epoch 00014 | Loss 0.8586 | Train MSE 0.8586 \n",
      "Epoch 00015 | Loss 0.7963 | Train MSE 0.7963 \n",
      "Epoch 00016 | Loss 0.6840 | Train MSE 0.6840 \n",
      "Epoch 00017 | Loss 0.5468 | Train MSE 0.5468 \n",
      "Epoch 00018 | Loss 0.4538 | Train MSE 0.4538 \n",
      "Epoch 00019 | Loss 0.4210 | Train MSE 0.4210 \n",
      "Epoch 00020 | Loss 0.4041 | Train MSE 0.4041 \n",
      "Epoch 00021 | Loss 0.3771 | Train MSE 0.3771 \n",
      "Epoch 00022 | Loss 0.3408 | Train MSE 0.3408 \n",
      "Epoch 00023 | Loss 0.2936 | Train MSE 0.2936 \n",
      "Epoch 00024 | Loss 0.2573 | Train MSE 0.2573 \n",
      "Epoch 00025 | Loss 0.2299 | Train MSE 0.2299 \n",
      "Epoch 00026 | Loss 0.2003 | Train MSE 0.2003 \n",
      "Epoch 00027 | Loss 0.1652 | Train MSE 0.1652 \n",
      "Epoch 00028 | Loss 0.1337 | Train MSE 0.1337 \n",
      "Epoch 00029 | Loss 0.1177 | Train MSE 0.1177 \n",
      "Epoch 00030 | Loss 0.1192 | Train MSE 0.1192 \n",
      "Epoch 00031 | Loss 0.1307 | Train MSE 0.1307 \n",
      "Epoch 00032 | Loss 0.1415 | Train MSE 0.1415 \n",
      "Epoch 00033 | Loss 0.1440 | Train MSE 0.1440 \n",
      "Epoch 00034 | Loss 0.1355 | Train MSE 0.1355 \n",
      "Epoch 00035 | Loss 0.1177 | Train MSE 0.1177 \n",
      "Epoch 00036 | Loss 0.0953 | Train MSE 0.0953 \n",
      "Epoch 00037 | Loss 0.0745 | Train MSE 0.0745 \n",
      "Epoch 00038 | Loss 0.0600 | Train MSE 0.0600 \n",
      "Epoch 00039 | Loss 0.0533 | Train MSE 0.0533 \n",
      "Epoch 00040 | Loss 0.0529 | Train MSE 0.0529 \n",
      "Epoch 00041 | Loss 0.0552 | Train MSE 0.0552 \n",
      "Epoch 00042 | Loss 0.0567 | Train MSE 0.0567 \n",
      "Epoch 00043 | Loss 0.0557 | Train MSE 0.0557 \n",
      "Epoch 00044 | Loss 0.0519 | Train MSE 0.0519 \n",
      "Epoch 00045 | Loss 0.0465 | Train MSE 0.0465 \n",
      "Epoch 00046 | Loss 0.0407 | Train MSE 0.0407 \n",
      "Epoch 00047 | Loss 0.0356 | Train MSE 0.0356 \n",
      "Epoch 00048 | Loss 0.0319 | Train MSE 0.0319 \n",
      "Epoch 00049 | Loss 0.0297 | Train MSE 0.0297 \n",
      "Test accuracy 0.7365\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training with DGL built-in RGCN module.')\n",
    "\n",
    "# load and preprocess dataset\n",
    "data = AIFBDataset()\n",
    "\n",
    "g = data[0]\n",
    "g = g.int().to(device)\n",
    "num_rels = len(g.canonical_etypes)\n",
    "category = data.predict_category\n",
    "labels = g.nodes[category].data.pop('labels')\n",
    "train_mask = g.nodes[category].data.pop('train_mask')\n",
    "test_mask = g.nodes[category].data.pop('test_mask')\n",
    "# calculate normalization weight for each edge, and find target category and node id\n",
    "for cetype in g.canonical_etypes:\n",
    "    g.edges[cetype].data['norm'] = dgl.norm_by_dst(g, cetype).unsqueeze(1)\n",
    "category_id = g.ntypes.index(category)\n",
    "g = dgl.to_homogeneous(g, edata=['norm'])\n",
    "node_ids = torch.arange(g.num_nodes()).to(device)\n",
    "target_idx = node_ids[g.ndata[dgl.NTYPE] == category_id]\n",
    "# create RGCN model    \n",
    "in_size = g.num_nodes() # featureless with one-hot encoding\n",
    "# out_size = data.num_classes\n",
    "out_size = 1 # regression\n",
    "model = RGCN(in_size, 16, out_size, num_rels).to(device)\n",
    "\n",
    "train(g, target_idx, labels, train_mask, model)\n",
    "mse_err = evaluate(g, target_idx, labels, test_mask, model)\n",
    "print(\"Test accuracy {:.4f}\".format(mse_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "data = AIFBDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = data[0]\n",
    "g = g.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes[category].data['train_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with DGL built-in RGCN module.\n",
      "Done loading data from cached files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenmin/miniconda3/envs/graph/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([140])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m out_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# regression\u001b[39;00m\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m RGCN(in_size, \u001b[38;5;241m16\u001b[39m, out_size, num_rels)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m mse_err \u001b[38;5;241m=\u001b[39m evaluate(g, target_idx, labels, test_mask, model)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(mse_err))\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(g, target_idx, labels, train_mask, model)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fcn(logits[train_idx], labels[train_idx])\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy(logits[train_idx]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), labels[train_idx])\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training with DGL built-in RGCN module.')\n",
    "\n",
    "# load and preprocess dataset\n",
    "data = AIFBDataset()\n",
    "\n",
    "g = data[0]\n",
    "g = g.int().to(device)\n",
    "num_rels = len(g.canonical_etypes)\n",
    "category = data.predict_category\n",
    "labels = g.nodes[category].data.pop('labels')\n",
    "train_mask = g.nodes[category].data.pop('train_mask')\n",
    "test_mask = g.nodes[category].data.pop('test_mask')\n",
    "# calculate normalization weight for each edge, and find target category and node id\n",
    "for cetype in g.canonical_etypes:\n",
    "    g.edges[cetype].data['norm'] = dgl.norm_by_dst(g, cetype).unsqueeze(1)\n",
    "category_id = g.ntypes.index(category)\n",
    "g = dgl.to_homogeneous(g, edata=['norm'])\n",
    "node_ids = torch.arange(g.num_nodes()).to(device)\n",
    "target_idx = node_ids[g.ndata[dgl.NTYPE] == category_id]\n",
    "# create RGCN model    \n",
    "in_size = g.num_nodes() # featureless with one-hot encoding\n",
    "# out_size = data.num_classes\n",
    "out_size = 1 # regression\n",
    "model = RGCN(in_size, 16, out_size, num_rels).to(device)\n",
    "\n",
    "train(g, target_idx, labels, train_mask, model)\n",
    "mse_err = evaluate(g, target_idx, labels, test_mask, model)\n",
    "print(\"Test accuracy {:.4f}\".format(mse_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
